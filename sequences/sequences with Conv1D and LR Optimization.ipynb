{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Sequence prediction with CNN and LR optimization***\n",
    "\n",
    "dataset is daily minimum temperatures in Melbourne from 1981 to 1990.\n",
    "[Daily Minimum Temperatures in Melbourne](https://github.com/jbrownlee/Datasets/blob/master/daily-min-temperatures.csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56XEQOGknrAk",
    "outputId": "6b8ab5ea-ed49-40d2-a27c-d2ba08710ba0"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by looking at the structure of the csv that contains the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header looks like this:\n",
      "\n",
      "\"Date\",\"Temp\"\n",
      "\n",
      "First data point looks like this:\n",
      "\n",
      "\"1981-01-01\",20.7\n",
      "\n",
      "Second data point looks like this:\n",
      "\n",
      "\"1981-01-02\",17.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEMPERATURES_CSV = 'd:/data/daily-min-temperatures.csv'\n",
    "\n",
    "with open(TEMPERATURES_CSV, 'r') as csvfile:\n",
    "    print(f\"Header looks like this:\\n\\n{csvfile.readline()}\")    \n",
    "    print(f\"First data point looks like this:\\n\\n{csvfile.readline()}\")\n",
    "    print(f\"Second data point looks like this:\\n\\n{csvfile.readline()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each data point is composed of the date and the recorded minimum temperature for that date.\n",
    "\n",
    "\n",
    "In the first exercise you will code a function to read the data from the csv but for now run the next cell to load a helper function to plot the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sLl52leVp5wU"
   },
   "outputs": [],
   "source": [
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the raw data\n",
    "Now you need to read the data from the csv file. To do so, complete the `parse_data_from_file` function.\n",
    "\n",
    "A couple of things to note:\n",
    "\n",
    "- You should omit the first line as the file contains headers.\n",
    "- There is no need to save the data points as numpy arrays, regular lists is fine.\n",
    "- To read from csv files use `csv.reader` by passing the appropriate arguments.\n",
    "- `csv.reader` returns an iterable that returns each row in every iteration. So the temperature can be accessed via row[1] and the date can be discarded.\n",
    "- The `times` list should contain every timestep (starting at zero), which is just a sequence of ordered numbers with the same length as the `temperatures` list.\n",
    "- The values of the `temperatures` should be of `float` type. You can use Python's built-in `float` function to ensure this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "NcG9r1eClbTh",
    "outputId": "7acf6ba9-e852-4f06-e06e-b0ff1b9e2ddd"
   },
   "outputs": [],
   "source": [
    "def parse_data_from_file(filename):\n",
    "    \n",
    "    times = []\n",
    "    temperatures = []\n",
    "\n",
    "    with open(filename) as csvfile:\n",
    "       \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)\n",
    "\n",
    "        for record in reader:\n",
    "\n",
    "            temperatures.append(float(record[1]))\n",
    " \n",
    "        times=list(range(0,len(temperatures)))\n",
    "    \n",
    "        print(type(times))\n",
    "    \n",
    "    print(len(times), len(temperatures))\n",
    "            \n",
    "    return times, temperatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will use your function to compute the `times` and `temperatures` and will save these as numpy arrays within the `G` dataclass. This cell will also plot the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/daily-min-temperatures.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test your function and save all \"global\" variables within the G class (G stands for global)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mG\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     TEMPERATURES_CSV \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/daily-min-temperatures.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m     times, temperatures \u001b[38;5;241m=\u001b[39m parse_data_from_file(TEMPERATURES_CSV)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mG\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mG\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     TEMPERATURES_CSV \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/daily-min-temperatures.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     times, temperatures \u001b[38;5;241m=\u001b[39m \u001b[43mparse_data_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEMPERATURES_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     TIME \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(times)\n\u001b[0;32m      7\u001b[0m     SERIES \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(temperatures)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mparse_data_from_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      3\u001b[0m times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m temperatures \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m      8\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(csvfile, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mnext\u001b[39m(reader)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/daily-min-temperatures.csv'"
     ]
    }
   ],
   "source": [
    "# Test your function and save all \"global\" variables within the G class (G stands for global)\n",
    "@dataclass\n",
    "class G:\n",
    "    TEMPERATURES_CSV = './data/daily-min-temperatures.csv'\n",
    "    times, temperatures = parse_data_from_file(TEMPERATURES_CSV)\n",
    "    TIME = np.array(times)\n",
    "    SERIES = np.array(temperatures)\n",
    "    SPLIT_TIME = 2500\n",
    "    WINDOW_SIZE = 64\n",
    "    BATCH_SIZE = 256\n",
    "    SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(G.TIME, G.SERIES)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<div>\n",
    "<img src=\"images/temp-series.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "Since you already coded the `train_val_split` and `windowed_dataset` functions during past week's assignments, this time they are provided for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(time, series, time_step=G.SPLIT_TIME):\n",
    "\n",
    "    time_train = time[:time_step]\n",
    "    series_train = series[:time_step]\n",
    "    time_valid = time[time_step:]\n",
    "    series_valid = series[time_step:]\n",
    "\n",
    "    return time_train, series_train, time_valid, series_valid\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "time_train, series_train, time_valid, series_valid = train_val_split(G.TIME, G.SERIES)\n",
    "print ('First 3 [time] [train]  ', time_train[0:3], series_train[0:3])\n",
    "print ('All       :',np.shape(G.TIME))\n",
    "print ('Train     :',np.shape(time_train))\n",
    "print ('Validation:',np.shape(time_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create windowed dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJwUUZscnG38"
   },
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE, shuffle_buffer=G.SHUFFLE_BUFFER_SIZE):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    #ds = ds.shuffle(shuffle_buffer)  # uncomment shuffle for prod\n",
    "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Apply the transformation to the training set\n",
    "train_set = windowed_dataset(series_train, window_size=64, batch_size=256, shuffle_buffer=1000)\n",
    "print ('train set shape: ',np.shape(train_set))\n",
    "print (type(train_set),'\\n')\n",
    "for elem in train_set:\n",
    "    print(elem[0],'\\n\\n')\n",
    "    print('current data shape: ',np.shape(elem[0])) \n",
    "    x=tf.expand_dims(elem[0], axis=-1)\n",
    "    print('need this shape for model: ',np.shape(x))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model architecture\n",
    "\n",
    "Now that you have a function that will process the data before it is fed into your neural network for training, it is time to define your layer architecture. Just as in last week's assignment you will do the layer definition and compilation in two separate steps. Begin by completing the `create_uncompiled_model` function below.\n",
    "\n",
    "This is done so you can reuse your model's layers for the learning rate adjusting and the actual training.\n",
    "\n",
    "Hint:\n",
    "\n",
    "- No `Lambda` layers are required\n",
    "- Use a combination of `Conv1D` and `LSTM` layers followed by `Dense` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uncompiled_model():\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "         tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]), # adds the required depth dim\n",
    "         tf.keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding='causal', activation='relu',input_shape=(64,1) ),\n",
    "         tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "         tf.keras.layers.LSTM(64),\n",
    "         tf.keras.layers.Dense(1)\n",
    "    ]) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test your uncompiled model\n",
    "print(\"window:\",G.WINDOW_SIZE)\n",
    "print(\"train_set:\",train_set,\"\\n\")\n",
    "print(\"train_set shape: \",np.shape(train_set))\n",
    "\n",
    "uncompiled_model = create_uncompiled_model()\n",
    "uncompiled_model.summary()\n",
    "\n",
    "for element in train_set:\n",
    "    uncompiled_model.predict(element[0])\n",
    "\n",
    "try:\n",
    "    uncompiled_model.predict(train_set)\n",
    "except:\n",
    "    print(\"\\Fail\")\n",
    "else:\n",
    "    print(\"\\nSuccess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the learning rate - (Optional Exercise)\n",
    "\n",
    "As you saw in the lecture you can leverage Tensorflow's callbacks to dinamically vary the learning rate during training. This can be helpful to get a better sense of which learning rate better acommodates to the problem at hand.\n",
    "\n",
    "**Notice that this is only changing the learning rate during the training process to give you an idea of what a reasonable learning rate is and should not be confused with selecting the best learning rate, this is known as hyperparameter optimization and it is outside the scope of this course.**\n",
    "\n",
    "For the optimizers you can try out:\n",
    "\n",
    "- tf.keras.optimizers.Adam\n",
    "- tf.keras.optimizers.SGD with a momentum of 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(dataset):\n",
    "    \n",
    "    model = create_uncompiled_model()\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=5e-5,momentum=.9)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.Huber(),\n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"mae\"]) \n",
    "    \n",
    "    history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training with dynamic LR\n",
    "lr_history = adjust_learning_rate(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "vVcKmg7Q_7rD",
    "outputId": "27cf16ae-eb85-47c3-fc86-18e72e528619"
   },
   "outputs": [],
   "source": [
    "plt.semilogx(lr_history.history[\"lr\"], lr_history.history[\"loss\"])\n",
    "plt.axis([1e-4, 10, 0, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "Now that you have trained the model while varying the learning rate, it is time to do the actual training that will be used to forecast the time series. For this complete the `create_model` function below.\n",
    "\n",
    "Notice that you are reusing the architecture you defined in the `create_uncompiled_model` earlier. Now you only need to compile this model using the appropriate loss, optimizer (and learning rate).\n",
    "\n",
    "Hints:\n",
    "\n",
    "- The training should be really quick so if you notice that each epoch is taking more than a few seconds, consider trying a different architecture.\n",
    "\n",
    "\n",
    "- If after the first epoch you get an output like this: loss: nan - mae: nan it is very likely that your network is suffering from exploding gradients. This is a common problem if you used SGD as optimizer and set a learning rate that is too high. If you encounter this problem consider lowering the learning rate or using Adam with the default learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = create_uncompiled_model()  #reuse model defined above   \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=2e-3 ),\n",
    "              metrics=[\"mae\"])  \n",
    "    \n",
    "\n",
    "history = model.fit(train_set, epochs=50,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the forecast\n",
    "\n",
    "Now it is time to evaluate the performance of the forecast. For this you can use the `compute_metrics` function that you coded in a previous assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_series, forecast):\n",
    "    \n",
    "    mse = tf.keras.metrics.mean_squared_error(true_series, forecast).numpy()\n",
    "    mae = tf.keras.metrics.mean_absolute_error(true_series, forecast).numpy()\n",
    "\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point only the model that will perform the forecast is ready but you still need to compute the actual forecast.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster model forecasts\n",
    "\n",
    "In the previous week you saw a faster approach compared to using a for loop to compute the forecasts for every point in the sequence. Remember that this faster approach uses batches of data. \n",
    "\n",
    "The code to implement this is provided in the `model_forecast` below. Notice that the code is very similar to the one in the `windowed_dataset` function with the differences that:\n",
    "- The dataset is windowed using `window_size` rather than `window_size + 1`\n",
    "- No shuffle should be used\n",
    "- No need to split the data into features and labels\n",
    "- A model is used to predict batches of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XwGrf-A_wF0"
   },
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(32).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "PrktQX3hKYex",
    "outputId": "1914662d-6bdd-4e17-8697-8f5a29e89b87"
   },
   "outputs": [],
   "source": [
    "# Compute the forecast for all the series\n",
    "rnn_forecast = model_forecast(model, G.SERIES, G.WINDOW_SIZE).squeeze()\n",
    "\n",
    "# Slice the forecast to get only the predictions for the validation set\n",
    "rnn_forecast = rnn_forecast[G.SPLIT_TIME - G.WINDOW_SIZE:-1]\n",
    "\n",
    "\n",
    "# Plot the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, series_valid)\n",
    "plot_series(time_valid, rnn_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, mae = compute_metrics(series_valid, rnn_forecast)\n",
    "\n",
    "print(f\"mse: {mse:.2f}, mae: {mae:.2f} for forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To pass this assignment your forecast should achieve a MSE of 6 or less and a MAE of 2 or less.**\n",
    "\n",
    "- If your forecast didn't achieve this threshold try re-training your model with a different architecture (you will need to re-run both `create_uncompiled_model` and `create_model` functions) or tweaking the optimizer's parameters.\n",
    "\n",
    "\n",
    "- If your forecast did achieve this threshold run the following cell to save the model in a HDF5 file which will be used for grading and after doing so, submit your assigment for grading.\n",
    "\n",
    "\n",
    "- This environment includes a dummy `my_model.h5` file which is just a dummy model trained for one epoch. **To replace this file with your actual model you need to run the next cell before submitting for grading.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model into a HDF5 file\n",
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this week's assignment!**\n",
    "\n",
    "You have successfully implemented a neural network capable of forecasting time series leveraging a combination of Tensorflow's layers such as Convolutional and LSTMs! This resulted in a forecast that surpasses all the ones you did previously.\n",
    "\n",
    "**By finishing this assignment you have finished the specialization! Give yourself a pat on the back!!!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
